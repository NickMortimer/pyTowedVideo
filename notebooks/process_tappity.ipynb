{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import glob\n",
    "from pyproj import Geod\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "usbl = glob.glob('S:\\HSV\\Log Files\\clean\\*.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "usbl = pd.concat([pd.read_csv(ub,index_col='timestamp',parse_dates=['timestamp']) for ub in usbl])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processs_keys(item):\n",
    "    return {'timestamp':item.timestamp,'keytype':item['data']['keytype'],'keyname':item['data']['keyname']}\n",
    "\n",
    "def process_data(file):\n",
    "    data = pd.read_json(file,lines=True)\n",
    "    keys = pd.DataFrame(list(data[data.message.str.endswith('keystroke.eng')].apply(processs_keys,axis=1)))\n",
    "    keys.set_index('timestamp',inplace=True)\n",
    "    keys =pd.pivot_table(keys, values='keyname', index=['timestamp'],columns=['keytype'],aggfunc='first')\n",
    "    benthos =pd.concat([keys['benthic'].resample('1S').first().ffill(),keys['substrate'].resample('1S').first().ffill()], axis=1)\n",
    "    cols = ['depth_meters','longitude','latitude']\n",
    "    benthos =benthos.join(usbl[cols])\n",
    "    benthos[cols] = benthos[cols].interpolate()\n",
    "    benthos = benthos[~benthos.longitude.isna()]\n",
    "    geod = Geod(ellps='WGS84')\n",
    "    azimuth1, azimuth2, distance = geod.inv(benthos.longitude.values[:-1], benthos.latitude.values[:-1], benthos.longitude.values[1:], benthos.latitude.values[1:])\n",
    "    benthos['distance'] =np.append(distance,0)\n",
    "    benthos =benthos.join(keys.loc[~keys.animal.isna(),'animal'],how='outer').sort_index()\n",
    "    benthos['benthic'] = benthos['benthic'].ffill()\n",
    "    benthos['substrate'] = benthos['substrate'].ffill()\n",
    "    benthos[cols] = benthos[cols].ffill()\n",
    "    benthos.loc[benthos.distance.isna(),'distance'] =0\n",
    "    benthos['station']=station =int(file.split('_')[7])\n",
    "    benthos.index.name = 'timestamp'\n",
    "    return benthos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S:/HSV/Tappity\\TAPPITY_NDR2019_2601_2019_03_26_03_48_21.json\n",
      "S:/HSV/Tappity\\TAPPITY_NDR2019_2803_2019_03_28_06_21_28.json\n",
      "S:/HSV/Tappity\\TAPPITY_NDR2019_2812_2019_03_28_07_01_14.json\n",
      "S:/HSV/Tappity\\TAPPITY_NDR2019_2818_2019_03_28_04_57_47.json\n",
      "S:/HSV/Tappity\\TAPPITY_NDR2019_2818_2019_03_28_04_59_15.json\n",
      "S:/HSV/Tappity\\TAPPITY_NDR2019_2904_2019_03_29_04_23_26.json\n",
      "S:/HSV/Tappity\\TAPPITY_NDR2019_2905_2019_03_29_00_33_11.json\n",
      "S:/HSV/Tappity\\TAPPITY_NDR2019_2906_2019_03_29_03_23_41.json\n",
      "S:/HSV/Tappity\\TAPPITY_NDR2019_2907_2019_03_29_01_06_32.json\n",
      "S:/HSV/Tappity\\TAPPITY_NDR2019_2909_2019_03_29_02_00_22.json\n",
      "S:/HSV/Tappity\\TAPPITY_NDR2019_2910_2019_03_29_02_43_48.json\n",
      "S:/HSV/Tappity\\TAPPITY_NDR2019_3014_2019_03_29_23_37_00.json\n",
      "S:/HSV/Tappity\\TAPPITY_NDR2019_3015_2019_03_30_00_20_30.json\n",
      "S:/HSV/Tappity\\TAPPITY_NDR2019_3016_2019_03_30_03_52_04.json\n",
      "S:/HSV/Tappity\\TAPPITY_NDR2019_3017_2019_03_30_03_16_12.json\n",
      "S:/HSV/Tappity\\TAPPITY_NDR2019_3019_2019_03_30_01_11_37.json\n",
      "S:/HSV/Tappity\\TAPPITY_NDR2019_3020_2019_03_30_02_34_26.json\n",
      "S:/HSV/Tappity\\TAPPITY_NDR2019_3023_2019_03_30_01_56_10.json\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "files = glob.glob('S:/HSV/Tappity/*.json')\n",
    "usbl = glob.glob('S:\\HSV\\Log Files\\clean\\*.csv')\n",
    "usbl = pd.concat([pd.read_csv(ub,index_col='timestamp',parse_dates=['timestamp']) for ub in usbl])\n",
    "ub = usbl.resample('1S').first()\n",
    "\n",
    "def process_data(file):\n",
    "    data = pd.read_json(file,lines=True)\n",
    "    keys = pd.DataFrame(list(data[data.message.str.endswith('keystroke.eng')].apply(processs_keys,axis=1)))\n",
    "    keys.set_index('timestamp',inplace=True)\n",
    "    keys =pd.pivot_table(keys, values='keyname', index=['timestamp'],columns=['keytype'],aggfunc='first')\n",
    "    benthos =pd.concat([keys['benthic'].resample('1S').first().ffill(),keys['substrate'].resample('1S').first().ffill()], axis=1)\n",
    "    benthos['station']=station =int(file.split('_')[2])\n",
    "    benthos.index.name = 'timestamp'\n",
    "    return benthos\n",
    "\n",
    "\n",
    "for file in files:\n",
    "    print(file)\n",
    "    data = process_data(file)\n",
    "    x =data.join(ub).interpolate()\n",
    "    x.to_csv(file.replace('.json','.csv'),index=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
